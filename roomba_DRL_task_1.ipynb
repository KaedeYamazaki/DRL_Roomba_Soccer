{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naoNGHQhfizQ",
        "outputId": "b16d70a0-0595-449e-81e2-a9560ac1e050"
      },
      "outputs": [],
      "source": [
        "# if str(get_ipython()).startswith('<google.colab.'):\n",
        "#     !pip install gymnasium\n",
        "#     !pip install pybullet\n",
        "#     !git clone https://github.com/bulletphysics/bullet3.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7uNi3h4fnJt",
        "outputId": "cab7c733-b590-4d1b-c3eb-6fdb821e1a81"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "from gymnasium.wrappers import RecordVideo\n",
        "import pybullet\n",
        "import pybullet_data\n",
        "\n",
        "from PIL import Image\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "from torch import nn, optim\n",
        "import torch\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation\n",
        "%matplotlib inline\n",
        "\n",
        "from IPython.display import HTML\n",
        "\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qg7TIA70GUT4"
      },
      "source": [
        "# Hyper parameter settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4w-ggZglGccN"
      },
      "outputs": [],
      "source": [
        "ENV = \"myEnv-v0\"\n",
        "NUM_EPISODES = 10000  # 最大試行回数\n",
        "MAX_STEPS = 1000  # 1試行のstep数\n",
        "NUM_MEANS = 10  # step数あたりの平均を取る\n",
        "\n",
        "sum_reward = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwPdIPpmXHuV"
      },
      "source": [
        "# Visualize_sim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBakFRnkXIC5"
      },
      "outputs": [],
      "source": [
        "class visualize_sim:\n",
        "    def make_mp4video(self, filename, frame):\n",
        "      video_dir = 'video'\n",
        "      os.makedirs(video_dir, exist_ok=True)\n",
        "      video_path = os.path.join(video_dir, filename)\n",
        "\n",
        "      frames = np.array(frame)\n",
        "      height, width, _ = frames[0].shape\n",
        "      fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "      video = cv2.VideoWriter(video_path, fourcc, 30, (width, height))\n",
        "\n",
        "      for frame in frames:\n",
        "          video.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "      video.release()\n",
        "      cv2.destroyAllWindows()\n",
        "\n",
        "    def make_gif(self,filename,frame):\n",
        "      images =[]\n",
        "\n",
        "      for im in frame:\n",
        "        img = Image.fromarray(im)\n",
        "        images.append(img)\n",
        "      images[0].save(filename,save_all=True, append_images=images[:], optimize=False, duration=40, loop=0)\n",
        "      images.clear()\n",
        "\n",
        "    def display_video(self,frames):\n",
        "      plt.figure(figsize=(8, 8), dpi=50)\n",
        "      patch = plt.imshow(frames[0], cmap='gray')\n",
        "      plt.axis('off')\n",
        "\n",
        "      def animate(i):\n",
        "          patch.set_data(frames[i])\n",
        "\n",
        "      anim = animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval=50)\n",
        "      display(HTML(anim.to_jshtml(default_mode='once')))\n",
        "      plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4keosX8d4ja"
      },
      "source": [
        "# Simulator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U92-7kf6fx2l"
      },
      "outputs": [],
      "source": [
        "class RoombaSimulator:\n",
        "  def __init__(self):\n",
        "    pybullet.connect(pybullet.DIRECT)\n",
        "    pybullet.resetSimulation()\n",
        "    pybullet.setAdditionalSearchPath(pybullet_data.getDataPath())\n",
        "    pybullet.setGravity(0,0,-9.8)\n",
        "    timestep = 1. / 50. # 制御周期 20ms仮定\n",
        "    pybullet.setTimeStep(timestep)\n",
        "    floor = pybullet.loadURDF(\"plane.urdf\")\n",
        "\n",
        "    # Roomba 初期位置\n",
        "    self.roomba_startposition = [0,0,0.05]  # x,y,z\n",
        "    self.roomba_startorient = pybullet.getQuaternionFromEuler([0,0,3.14])\n",
        "    self.roomba600 = pybullet.loadURDF(\"/urdf/Roomba600.urdf\",self.roomba_startposition, self.roomba_startorient)\n",
        "    self.get_urdf_info(self.roomba600)\n",
        "\n",
        "\n",
        "    # ball 初期位置\n",
        "    self.ball_startposition = [-0.5,0,0.0975]\n",
        "    self.ball_startorient = pybullet.getQuaternionFromEuler([0,0,3.14])\n",
        "    self.ball = pybullet.loadURDF(\"/urdf/soccerball_fri=0.0001.urdf\",self.ball_startposition, self.ball_startorient)\n",
        "    self.get_urdf_info(self.ball)\n",
        "\n",
        "    # Goalの設定\n",
        "    self.goal_startposition = [-1.5,0,0.1]\n",
        "    self.goal_startorient = pybullet.getQuaternionFromEuler([0,0,0])\n",
        "    self.goal = pybullet.loadURDF(\"/urdf/Goal.urdf\",self.goal_startposition, self.goal_startorient)\n",
        "    self.get_urdf_info(self.goal)\n",
        "\n",
        "    #Courtの設定\n",
        "    self.court_startposition = [0,-2,0.1]\n",
        "    self.court_startorient = pybullet.getQuaternionFromEuler([0,0,0])\n",
        "    self.court = pybullet.loadURDF(\"/urdf/Court.urdf\",self.court_startposition, self.court_startorient)\n",
        "\n",
        "    self.cameraIdx = 6 # カメラリンク\n",
        "    self.cameraTargetIdx = 8 # 注視点用の仮想的なリンク\n",
        "\n",
        "    self.video_index = 0\n",
        "\n",
        "\n",
        "    # 記録用のフレーム\n",
        "    self.FPS = []\n",
        "    self.OVH = []\n",
        "    self.SIDE = []\n",
        "\n",
        "    self.visim = visualize_sim()\n",
        "\n",
        "  def get_urdf_info(self,instance):\n",
        "    print(instance)\n",
        "    num_joints = pybullet.getNumJoints(instance)\n",
        "    link_indices = [i for i in range(-1, num_joints)]\n",
        "\n",
        "    print(\"num_joints:\",num_joints)\n",
        "    print(\"link_indices:\",link_indices)\n",
        "\n",
        "    # ジョイントの情報を取得\n",
        "    for i in range(pybullet.getNumJoints(instance)):\n",
        "        print(pybullet.getJointInfo(instance, i))\n",
        "\n",
        "\n",
        "  def init_position(self):\n",
        "    pybullet.resetBasePositionAndOrientation(self.roomba600, self.roomba_startposition, self.roomba_startorient)\n",
        "    pybullet.resetBasePositionAndOrientation(self.ball, self.ball_startposition, self.ball_startorient)\n",
        "    pybullet.resetBasePositionAndOrientation(self.goal, self.goal_startposition, self.goal_startorient)\n",
        "    pybullet.resetBasePositionAndOrientation(self.court, self.court_startposition, self.court_startorient)\n",
        "    self.FPS.clear()\n",
        "    self.OVH.clear()\n",
        "    self.SIDE.clear()\n",
        "\n",
        "  def random_ball_position(self):\n",
        "    pybullet.resetBasePositionAndOrientation(self.roomba600, self.roomba_startposition, self.roomba_startorient)\n",
        "    pybullet.resetBasePositionAndOrientation(self.ball, [np.random.rand()*3-1.5, np.random.rand()*2-1.5, 0.0975], self.ball_startorient)\n",
        "    pybullet.resetBasePositionAndOrientation(self.goal, self.goal_startposition, self.goal_startorient)\n",
        "    pybullet.resetBasePositionAndOrientation(self.court, self.court_startposition, self.court_startorient)\n",
        "    self.FPS.clear()\n",
        "    self.OVH.clear()\n",
        "    self.SIDE.clear()\n",
        "\n",
        "  def random_initial_ball_velocity(self):\n",
        "    pybullet.resetBasePositionAndOrientation(self.roomba600, self.roomba_startposition, self.roomba_startorient)\n",
        "    pybullet.resetBasePositionAndOrientation(self.ball, self.ball_startposition, self.ball_startorient)\n",
        "    pybullet.resetBasePositionAndOrientation(self.goal, self.goal_startposition, self.goal_startorient)\n",
        "    pybullet.resetBasePositionAndOrientation(self.court, self.court_startposition, self.court_startorient)\n",
        "\n",
        "    initial_velocity = [np.random.rand()*10, np.random.rand()*10-5, 0]  # X方向は0~2の範囲で，Y方向は-2~2の範囲で，Z軸は常に0\n",
        "    pybullet.resetBaseVelocity(self.ball, initial_velocity)\n",
        "    self.FPS.clear()\n",
        "    self.OVH.clear()\n",
        "    self.SIDE.clear()\n",
        "\n",
        "  def get_now_roomba_position(self):\n",
        "    return pybullet.getBasePositionAndOrientation(self.roomba600)[0]\n",
        "\n",
        "  def get_now_ball_position(self):\n",
        "    return pybullet.getBasePositionAndOrientation(self.ball)[0]\n",
        "\n",
        "  def get_now_goal_position(self):\n",
        "    return pybullet.getBasePositionAndOrientation(self.goal)[0]\n",
        "\n",
        "  def check_link_fallen(self, height_threshold = 0.2):\n",
        "\n",
        "    link_orientation = pybullet.getLinkState(self.roomba600, 0)[1] #クォータニオン\n",
        "    link_position = pybullet.getLinkState(self.roomba600, 7)[0]\n",
        "\n",
        "    return link_position[2] < height_threshold\n",
        "\n",
        "  def update(self, targetVelocity_right, targetVelocity_left):\n",
        "    pybullet.setJointMotorControl2(self.roomba600, 2, pybullet.VELOCITY_CONTROL, targetVelocity = targetVelocity_right ,force = 6.23)\n",
        "    pybullet.setJointMotorControl2(self.roomba600, 3, pybullet.VELOCITY_CONTROL, targetVelocity = targetVelocity_left ,force = 6.23)\n",
        "    pybullet.stepSimulation()\n",
        "\n",
        "  def touch_ball(self):\n",
        "    result = pybullet.getClosestPoints(self.ball, self.roomba600, distance=0, linkIndexA=-1, linkIndexB=0)\n",
        "    if result:\n",
        "      return True\n",
        "    else:\n",
        "      return False\n",
        "\n",
        "  def goal_ball(self):\n",
        "    result = pybullet.getClosestPoints(self.ball, self.goal, distance=0, linkIndexA=-1, linkIndexB=4)\n",
        "    if result:\n",
        "      return True\n",
        "    else:\n",
        "      return False\n",
        "\n",
        "  def ball_touch_court(self):\n",
        "    result_1 = pybullet.getClosestPoints(self.ball, self.court, distance=0, linkIndexA=-1, linkIndexB =-1)\n",
        "    result_2 = pybullet.getClosestPoints(self.ball, self.court, distance=0, linkIndexA=-1, linkIndexB = 0)\n",
        "    result_3 = pybullet.getClosestPoints(self.ball, self.court, distance=0, linkIndexA=-1, linkIndexB = 1)\n",
        "    result_4 = pybullet.getClosestPoints(self.ball, self.court, distance=0, linkIndexA=-1, linkIndexB = 2)\n",
        "    if result_1 or result_2 or result_3 or result_4:\n",
        "      return True\n",
        "    else:\n",
        "      return False\n",
        "\n",
        "  def roomba_touch_court(self):\n",
        "    for i in range(-1,3):\n",
        "      result_1 = pybullet.getClosestPoints(self.roomba600, self.court, distance=0, linkIndexA= 0, linkIndexB = i)\n",
        "      result_2 = pybullet.getClosestPoints(self.roomba600, self.court, distance=0, linkIndexA= 4, linkIndexB = i)\n",
        "      result_3 = pybullet.getClosestPoints(self.roomba600, self.court, distance=0, linkIndexA= 5, linkIndexB = i)\n",
        "      if result_1 or result_2 or result_3:\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "\n",
        "  def get_vision(self,rec_flag):\n",
        "\n",
        "    if rec_flag:\n",
        "      self.video_index += 1\n",
        "      # 俯瞰カメラ\n",
        "      camera_target_position = [0, 0, 0]  # カメラの焦点の位置\n",
        "      camera_distance = 3.0  # カメラからの距離\n",
        "      camera_yaw = 0  # カメラの水平方向の角度\n",
        "      camera_pitch = -90  # カメラの垂直方向の角度\n",
        "      camera_target_position[2] = 2  # カメラの高さを調整\n",
        "\n",
        "      view_matrix = pybullet.computeViewMatrixFromYawPitchRoll(camera_target_position, camera_distance, camera_yaw, camera_pitch, 0, 2)\n",
        "      projection_matrix = pybullet.computeProjectionMatrixFOV(fov=60, aspect=1.0, nearVal=0.1, farVal=100.0)\n",
        "      width, height, rgbImg, depthImg, segImg = pybullet.getCameraImage(width=720, height=720, viewMatrix=view_matrix, projectionMatrix=projection_matrix)\n",
        "      self.OVH.append(rgbImg)\n",
        "\n",
        "      # # 真横カメラ\n",
        "      # camera_target_position = [0, 0, 0]  # カメラの焦点の位置\n",
        "      # camera_distance = 3.0  # カメラからの距離\n",
        "      # camera_yaw = 90  # カメラの水平方向の角度\n",
        "      # camera_pitch = 0  # カメラの垂直方向の角度\n",
        "      # camera_target_position[2] = 0.5  # カメラの高さを調整\n",
        "\n",
        "      # view_matrix = pybullet.computeViewMatrixFromYawPitchRoll(camera_target_position, camera_distance, camera_yaw, camera_pitch, 0, 2)\n",
        "      # projection_matrix = pybullet.computeProjectionMatrixFOV(fov=60, aspect=1.0, nearVal=0.1, farVal=100.0)\n",
        "      # width, height, rgbImg, depthImg, segImg = pybullet.getCameraImage(width=1280, height=1280, viewMatrix=view_matrix, projectionMatrix=projection_matrix)\n",
        "      # self.SIDE.append(rgbImg)\n",
        "\n",
        "    # FPSカメラ\n",
        "    camera_link_pose = pybullet.getLinkState(self.roomba600, self.cameraIdx)[0] # カメラリンクの位置を取得\n",
        "    camera_target_link_pose = pybullet.getLinkState(self.roomba600, self.cameraTargetIdx)[0] # 注視点用の仮想的なリンクの位置を取得\n",
        "\n",
        "    fp_viewMatrix = pybullet.computeViewMatrix(cameraEyePosition=[camera_link_pose[0], camera_link_pose[1], camera_link_pose[2]],cameraTargetPosition=[camera_target_link_pose[0], camera_target_link_pose[1], camera_target_link_pose[2]],cameraUpVector=[0, 0, 1]) # カメラリンク -> 注視点用の仮想的なリンク方向のviewMatrixを取得\n",
        "    fp_projectionMatrix = pybullet.computeProjectionMatrixFOV(fov=45.0,aspect=1.0,nearVal=0.1,farVal=10) # カメラ情報を設定\n",
        "\n",
        "    width, height, rgbImg, depthImg, segImg = pybullet.getCameraImage(84,84, fp_viewMatrix, fp_projectionMatrix)\n",
        "\n",
        "    # if rec_flag:\n",
        "    #   self.FPS.append(rgbImg)\n",
        "\n",
        "    return rgbImg\n",
        "\n",
        "\n",
        "\n",
        "  def save_video(self):\n",
        "      fp_video_name = f'Roomba_FPS_{self.video_index}.mp4'\n",
        "      ovh_video_name = f'Roomba_OVH_{self.video_index}.mp4'\n",
        "      side_video_name = f'Roomba_SIDE_{self.video_index}.mp4'\n",
        "\n",
        "      # self.visim.make_mp4video(fp_video_name,self.FPS)\n",
        "      self.visim.make_mp4video(ovh_video_name,self.OVH)\n",
        "      # self.visim.make_mp4video(side_video_name,self.SIDE)\n",
        "\n",
        "  def display_video(self):\n",
        "      self.visim.display_video(self.FPS)\n",
        "      self.visim.display_video(self.OVH)\n",
        "      self.visim.display_video(self.SIDE)\n",
        "\n",
        "  def save_gif(self):\n",
        "    pass\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9Jw4fAWd4jb"
      },
      "source": [
        "# Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzeybF8Rd4jb"
      },
      "outputs": [],
      "source": [
        "class myEnv(gym.Env):\n",
        "    metadata = {'render.modes': ['human', 'rgb_array']}\n",
        "\n",
        "    def __init__(self,render_mode):\n",
        "        super(myEnv, self).__init__()\n",
        "        self.roomba_simulator = RoombaSimulator()\n",
        "        self.action_space = gym.spaces.Discrete(5)       # エージェントが取りうる離散的な行動空間を定義．連続値の場合：self.action_space = gym.spaces.Box(0, 10, (2,), np.float32)\n",
        "        self.observation_space = gym.spaces.Box(0, 255, (84, 84, 4), np.uint8)  # エージェントが受け取りうる観測空間を定義\n",
        "        self.render_mode = render_mode\n",
        "        self.episode_num = 0\n",
        "        self.rec_flag = False\n",
        "        self.rec_interval = NUM_EPISODES\n",
        "        # self.reward_range = ...       # 報酬の範囲[最小値と最大値]を定義\n",
        "\n",
        "    def ctrl(self, action):\n",
        "\n",
        "        if action == 0:\n",
        "            targetVelocity_right = 32.3 #rad/s 周速より求めた車輪の角速度\n",
        "            targetVelocity_left  = 32.3\n",
        "\n",
        "        elif action == 1: # 左超信地旋回\n",
        "            targetVelocity_right = 32.3\n",
        "            targetVelocity_left  = -32.3\n",
        "\n",
        "        elif action == 2: # 右超信地旋回\n",
        "            targetVelocity_right = -32.3\n",
        "            targetVelocity_left  = 32.3\n",
        "\n",
        "        elif action == 3: # 左緩旋回\n",
        "            targetVelocity_right = 32.3\n",
        "            targetVelocity_left  = 32.3/2\n",
        "\n",
        "        elif action == 4: # 右緩旋回\n",
        "            targetVelocity_right = 32.3/2\n",
        "            targetVelocity_left  = 32.3\n",
        "\n",
        "        return targetVelocity_right, targetVelocity_left\n",
        "\n",
        "\n",
        "    def reset_(self, seed=None, options=None):\n",
        "        # 環境を初期状態にする関数\n",
        "        # 初期状態をreturnする\n",
        "        super().reset(seed=seed)\n",
        "        self.roomba_simulator.init_position()\n",
        "\n",
        "        self.rec_flag = False\n",
        "        self.episode_num += 1\n",
        "\n",
        "        if self.episode_num % self.rec_interval == 0:\n",
        "          self.rec_flag = True\n",
        "\n",
        "\n",
        "        obs = self.roomba_simulator.get_vision(self.rec_flag)\n",
        "\n",
        "        ball_pos = self.roomba_simulator.get_now_ball_position()\n",
        "        goal_pos = self.roomba_simulator.get_now_goal_position()\n",
        "        roomba_pos = self.roomba_simulator.get_now_roomba_position()\n",
        "\n",
        "        self.pre_ball2goal_dis = np.sqrt((ball_pos[0] - goal_pos[0])**2+(ball_pos[1] - goal_pos[1])**2)\n",
        "        self.pre_roomba2ball_dis = np.sqrt((ball_pos[0] - roomba_pos[0])**2+(ball_pos[1] - roomba_pos[1])**2)\n",
        "\n",
        "        return obs.transpose([2, 0, 1])\n",
        "\n",
        "    def step_(self, action):\n",
        "        # 行動を受け取り行動後の状態をreturnする\n",
        "        targetVelocity_right,targetVelocity_left = self.ctrl(action)\n",
        "\n",
        "        self.roomba_simulator.update(targetVelocity_right,targetVelocity_left)\n",
        "\n",
        "        terminated  = self.roomba_simulator.goal_ball()\n",
        "\n",
        "        truncated_1 = self.roomba_simulator.check_link_fallen()\n",
        "        truncated_2 = self.roomba_simulator.ball_touch_court()\n",
        "        truncated_3 = self.roomba_simulator.roomba_touch_court()\n",
        "\n",
        "        if truncated_1 or truncated_2 or truncated_3: # 転倒，ボールが壁と接触，ルンバが壁と接触すると停止\n",
        "          truncated = True\n",
        "        else:\n",
        "          truncated = False\n",
        "\n",
        "        now_position = self.roomba_simulator.get_now_roomba_position()\n",
        "\n",
        "        obs = self.roomba_simulator.get_vision(self.rec_flag)\n",
        "\n",
        "        reward = self.cal_reward(terminated,truncated)\n",
        "\n",
        "        info = {'example':now_position}\n",
        "\n",
        "        return obs.transpose([2, 0, 1]), reward, terminated, truncated, info\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        # modeとしてhuman, rgb_array, ansiが選択可能\n",
        "        # humanなら描画し, rgb_arrayならそれをreturnし, ansiなら文字列をreturnする\n",
        "        if mode == 'rgb_array':\n",
        "            return self.roomba_simulator.get_vision()\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "    def cal_reward(self,terminated,truncated):\n",
        "        ball_pos = self.roomba_simulator.get_now_ball_position()\n",
        "        goal_pos = self.roomba_simulator.get_now_goal_position()\n",
        "        roomba_pos = self.roomba_simulator.get_now_roomba_position()\n",
        "\n",
        "\n",
        "        roomba2ball_dis = np.sqrt((ball_pos[0] - roomba_pos[0])**2+(ball_pos[1] - roomba_pos[1])**2)\n",
        "        ball2goal_dis = np.sqrt((ball_pos[0] - goal_pos[0])**2+(ball_pos[1] - goal_pos[1])**2)\n",
        "\n",
        "        if truncated:\n",
        "          reward_3 = -1\n",
        "        else:\n",
        "          reward_3 = 0\n",
        "\n",
        "\n",
        "        if terminated:\n",
        "          reward_4 = 1\n",
        "        else:\n",
        "          reward_4 = 0\n",
        "\n",
        "        self.pre_roomba2ball_dis = roomba2ball_dis\n",
        "        self.pre_ball2goal_dis = ball2goal_dis\n",
        "\n",
        "        return reward_3+reward_4\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # def close(self):\n",
        "    #     ...\n",
        "\n",
        "    # def seed(self, seed=None):\n",
        "    #     ...\n",
        "\n",
        "    def save_video(self):\n",
        "      if self.rec_flag:\n",
        "        self.roomba_simulator.save_video()\n",
        "      else:\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuTnT7ITd4jb",
        "outputId": "2f8b7a64-061d-49d3-cb72-63a8b85dca1f"
      },
      "outputs": [],
      "source": [
        "gym.envs.registration.register(id='myEnv-v0',entry_point=myEnv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6ECuGEAd4jb"
      },
      "source": [
        "# DQN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q42oWJSeVeqx",
        "outputId": "89249038-3762-46d3-a964-b2789955769c"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oboHMtbHVeqy"
      },
      "outputs": [],
      "source": [
        "class WarpFrame(gym.ObservationWrapper):\n",
        "    def __init__(self, env, width=84, height=84, grayscale=False, dict_space_key=None):\n",
        "        super().__init__(env)\n",
        "        self._width = width\n",
        "        self._height = height\n",
        "        self._grayscale = grayscale\n",
        "        self._key = dict_space_key\n",
        "        if self._grayscale:\n",
        "            num_colors = 1\n",
        "        else:\n",
        "            num_colors = 3\n",
        "\n",
        "        new_space = gym.spaces.Box(\n",
        "            low=0,\n",
        "            high=255,\n",
        "            shape=(self._height, self._width, num_colors),\n",
        "            dtype=np.uint8,\n",
        "        )\n",
        "        if self._key is None:\n",
        "            original_space = self.observation_space\n",
        "            self.observation_space = new_space\n",
        "        else:\n",
        "            original_space = self.observation_space.spaces[self._key]\n",
        "            self.observation_space.spaces[self._key] = new_space\n",
        "        assert original_space.dtype == np.uint8 and len(original_space.shape) == 3\n",
        "\n",
        "\"\"\"\n",
        "    PyTorchで扱いやすいように観測をChannel-Firstに変更しつつ, Torch.Tensorに変換する.\n",
        "\"\"\"\n",
        "class TorchFrame(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        height, width, channels = self.observation_space.shape\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=0,\n",
        "            high=255,\n",
        "            shape=(channels, height, width),\n",
        "            dtype=np.uint8,\n",
        "        )\n",
        "\n",
        "    def observation(self, obs):\n",
        "        return torch.as_tensor(obs.transpose([2, 0, 1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "siO3F5xGVeqy",
        "outputId": "552514ff-a26a-4f14-8a68-abac78b5d793"
      },
      "outputs": [],
      "source": [
        "def make_env():\n",
        "    env = gym.make(ENV, render_mode='rgb_array')\n",
        "    # env = WarpFrame(env=env)\n",
        "    env = TorchFrame(env)\n",
        "    return env\n",
        "# 実行する課題を設定\n",
        "env = make_env()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhYpSavcVeqy"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "   Prioritized Experience Replayを実現するためのメモリクラス.\n",
        "\"\"\"\n",
        "class PrioritizedReplayBuffer(object):\n",
        "    def __init__(self, buffer_size):\n",
        "        self.buffer_size = buffer_size\n",
        "        self.index = 0\n",
        "        self.buffer = []\n",
        "        self.priorities = np.zeros(buffer_size, dtype=np.float32)\n",
        "        self.priorities[0] = 1.0\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    # 経験をリプレイバッファに保存する． 経験は(obs, action, reward, next_obs, done)の5つ組を想定\n",
        "    def push(self, experience):\n",
        "        if len(self.buffer) < self.buffer_size:\n",
        "            self.buffer.append(experience)\n",
        "        else:\n",
        "            self.buffer[self.index] = experience\n",
        "\n",
        "        # 優先度は最初は大きな値で初期化しておき, 後でサンプルされた時に更新する\n",
        "        self.priorities[self.index] = self.priorities.max()\n",
        "        self.index = (self.index + 1) % self.buffer_size\n",
        "\n",
        "    def sample(self, batch_size, alpha=0.6, beta=0.4):\n",
        "        # 現在経験が入っている部分に対応する優先度を取り出し, サンプルする確率を計算\n",
        "        priorities = self.priorities[: self.buffer_size if len(self.buffer) == self.buffer_size else self.index]\n",
        "        priorities = priorities ** alpha\n",
        "        prob = priorities / priorities.sum()\n",
        "\n",
        "        # >> 演習: 確率probに従ってサンプルする経験のインデックスを用意しましょう\n",
        "        # ヒント: np.random.choice などが便利です\n",
        "        indices = np.random.choice(len(self.buffer), batch_size, p=prob)\n",
        "\n",
        "        # >> 演習: 上式の通りに重点サンプリングの補正のための重みを計算してみましょう\n",
        "        weights = (len(self.buffer) * prob[indices]) ** (-beta)\n",
        "        weights = weights / weights.max()\n",
        "\n",
        "        # 上でサンプルしたインデックスに基づいて経験をサンプルし, (obs, action, reward, next_obs, done)に分ける\n",
        "        obs, action, reward, next_obs, done = zip(*[self.buffer[i] for i in indices])\n",
        "\n",
        "        # あとで計算しやすいようにtorch.Tensorに変換して(obs, action, reward, next_obs, done, indices, weights)の7つ組を返す\n",
        "        return (torch.stack(obs),\n",
        "                torch.as_tensor(action),\n",
        "                torch.as_tensor(reward, dtype=torch.float32),\n",
        "                torch.stack(next_obs),\n",
        "                torch.as_tensor(done, dtype=torch.uint8),\n",
        "                indices,\n",
        "                torch.as_tensor(weights, dtype=torch.float32))\n",
        "\n",
        "    # 優先度を更新する. 優先度が極端に小さくなって経験が全く選ばれないということがないように, 微小値を加算しておく.\n",
        "    def update_priorities(self, indices, priorities):\n",
        "        self.priorities[indices] = priorities + 1e-4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yq9FOU1GVeqy"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "    Dueling Networkを用いたQ関数を実現するためのニューラルネットワークをクラスとして記述します.\n",
        "\"\"\"\n",
        "class CNNQNetwork(nn.Module):\n",
        "    def __init__(self, state_shape, n_action):\n",
        "        super(CNNQNetwork, self).__init__()\n",
        "        self.state_shape = state_shape\n",
        "        self.n_action = n_action\n",
        "        # Dueling Networkでも, 畳込み部分は共有する\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(state_shape[0], 32, kernel_size=8, stride=4),  # 1x84x84 -> 32x20x20\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),  # 32x20x20 -> 64x9x9\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),  # 64x9x9 -> 64x7x7\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Dueling Networkのための分岐した全結合層\n",
        "        # 状態価値\n",
        "        self.fc_state = nn.Sequential(\n",
        "            nn.Linear(3136, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 1)\n",
        "        )\n",
        "\n",
        "        # アドバンテージ\n",
        "        self.fc_advantage = nn.Sequential(\n",
        "            nn.Linear(3136, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, n_action)\n",
        "        )\n",
        "\n",
        "    def forward(self, obs):\n",
        "        feature = self.conv_layers(obs)\n",
        "        feature = feature.view(-1, 3136)  #　Flatten. 64x7x7　-> 3136\n",
        "\n",
        "        state_values = self.fc_state(feature)\n",
        "        advantage = self.fc_advantage(feature)\n",
        "\n",
        "        # 状態価値 + アドバンテージ で行動価値を計算しますが、安定化のためアドバンテージの（行動間での）平均を引きます\n",
        "        action_values = state_values + advantage - torch.mean(advantage, dim=1, keepdim=True)\n",
        "        return action_values\n",
        "\n",
        "    # epsilon-greedy. 確率epsilonでランダムに行動し, それ以外はニューラルネットワークの予測結果に基づいてgreedyに行動します.\n",
        "    def act(self, obs, epsilon):\n",
        "        if random.random() < epsilon:\n",
        "            action = random.randrange(self.n_action)\n",
        "        else:\n",
        "            # 行動を選択する時には勾配を追跡する必要がない\n",
        "            with torch.no_grad():\n",
        "                # action = torch.argmax(self.forward(obs.unsqueeze(0))).item()\n",
        "                action = torch.argmax(self.forward(obs)).item()\n",
        "        return action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3OsuQShsZVME"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "    リプレイバッファの宣言\n",
        "\"\"\"\n",
        "buffer_size = 50000  #　リプレイバッファに入る経験の最大数\n",
        "initial_buffer_size = 5000  # 学習を開始する最低限の経験の数\n",
        "replay_buffer = PrioritizedReplayBuffer(buffer_size)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "    ネットワークの宣言\n",
        "\"\"\"\n",
        "net = CNNQNetwork(env.observation_space.shape, n_action=env.action_space.n).to(device)\n",
        "target_net = CNNQNetwork(env.observation_space.shape, n_action=env.action_space.n).to(device)\n",
        "target_update_interval = 2000  # 学習安定化のために用いるターゲットネットワークの同期間隔\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "    オプティマイザとロス関数の宣言\n",
        "\"\"\"\n",
        "optimizer = optim.Adam(net.parameters(), lr=1e-4)  # オプティマイザはAdam\n",
        "loss_func = nn.SmoothL1Loss(reduction='none')  # ロスはSmoothL1loss（別名Huber loss）\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "    Prioritized Experience Replayのためのパラメータβ\n",
        "\"\"\"\n",
        "beta_begin = 0.4\n",
        "beta_end = 1.0\n",
        "beta_decay = NUM_EPISODES*0.8\n",
        "# beta_beginから始めてbeta_endまでbeta_decayかけて線形に増やす\n",
        "beta_func = lambda step: min(beta_end, beta_begin + (beta_end - beta_begin) * (step / beta_decay))\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "    探索のためのパラメータε\n",
        "\"\"\"\n",
        "epsilon_begin = 1.0\n",
        "epsilon_end = 0.01\n",
        "epsilon_decay = NUM_EPISODES*0.6\n",
        "# epsilon_beginから始めてepsilon_endまでepsilon_decayかけて線形に減らす\n",
        "epsilon_func = lambda step: max(epsilon_end, epsilon_begin - (epsilon_begin - epsilon_end) * (step / epsilon_decay))\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "    その他のハイパーパラメータ\n",
        "\"\"\"\n",
        "gamma = 0.99  #　割引率\n",
        "batch_size = 32\n",
        "n_episodes = 300  # 学習を行うエピソード数"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0FX8tFJVeqz"
      },
      "outputs": [],
      "source": [
        "def update(batch_size, beta):\n",
        "    obs, action, reward, next_obs, done, indices, weights = replay_buffer.sample(batch_size, beta)\n",
        "    obs, action, reward, next_obs, done, weights \\\n",
        "        = obs.float().to(device), action.to(device), reward.to(device), next_obs.float().to(device), done.to(device), weights.to(device)\n",
        "\n",
        "    #　ニューラルネットワークによるQ関数の出力から, .gatherで実際に選択した行動に対応する価値を集めてきます.\n",
        "    q_values = net(obs).gather(1, action.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "    # 目標値の計算なので勾配を追跡しない\n",
        "    with torch.no_grad():\n",
        "        # Double DQN.\n",
        "        # >> 演習: Double DQNのターゲット価値の計算を実装してみましょう\n",
        "        # ① 現在のQ関数でgreedyに行動を選択し,\n",
        "        greedy_action_next = torch.argmax(net(next_obs), dim=1)\n",
        "        # ②　対応する価値はターゲットネットワークのものを参照します.\n",
        "        q_values_next = target_net(next_obs).gather(1, greedy_action_next.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "    # ベルマン方程式に基づき, 更新先の価値を計算します.\n",
        "    # (1 - done)をかけているのは, ゲームが終わった後の価値は0とみなすためです.\n",
        "    target_q_values = reward + gamma * q_values_next * (1 - done)\n",
        "\n",
        "    # Prioritized Experience Replayのために, ロスに重み付けを行なって更新します.\n",
        "    optimizer.zero_grad()\n",
        "    loss = (weights * loss_func(q_values, target_q_values)).mean()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    #　TD誤差に基づいて, サンプルされた経験の優先度を更新します.\n",
        "    replay_buffer.update_priorities(indices, (target_q_values - q_values).abs().detach().cpu().numpy())\n",
        "\n",
        "    return loss.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3DxG8BOyd4jc"
      },
      "outputs": [],
      "source": [
        "class DQN:\n",
        "    def random_action(self):\n",
        "        action = np.random.randint(0, 4, (1,))\n",
        "        return action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61LI5LE1d4jc"
      },
      "source": [
        "# Learning Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sm6n5MGNd4jc"
      },
      "outputs": [],
      "source": [
        "class LearningAgent:\n",
        "    def __init__(self, env):\n",
        "        self.env = env\n",
        "        trigger = lambda episode_id: episode_id % 10 == 0  # 10エピソードごとに録画\n",
        "        # self.env = RecordVideo(env, video_folder=\"videos/\", episode_trigger=trigger)\n",
        "        # self.num_states = ...\n",
        "        # self.num_actions = ...\n",
        "\n",
        "\n",
        "        self.reward_mean = np.zeros(NUM_MEANS)  # NUM_MEANS試行分の報酬を格納し、平均報酬を求める．\n",
        "        self.rewards = 0\n",
        "\n",
        "    def run(self):\n",
        "        update_step = 0\n",
        "        for episode in range(NUM_EPISODES):  # 最大試行数分繰り返す\n",
        "            obs = self.env.reset_()  # 環境の初期化\n",
        "            self.rewards = 0\n",
        "\n",
        "\n",
        "            for step in range(MAX_STEPS):  # 1試行のループ\n",
        "                obs = torch.tensor(obs, dtype=torch.float, device=device)\n",
        "                # ここでDQNagentの推論結果を得る\n",
        "                action = net.act(obs.float().to(device), epsilon_func(episode))\n",
        "\n",
        "                observation_next, reward, terminated, truncated, info   = self.env.step_(action)\n",
        "                observation_next = torch.tensor(observation_next, dtype=torch.float, device=device)\n",
        "\n",
        "                self.rewards += reward\n",
        "\n",
        "                # リプレイバッファに経験を蓄積\n",
        "                replay_buffer.push([obs, action, reward, observation_next, terminated])\n",
        "                obs = observation_next\n",
        "\n",
        "                # ネットワークを更新\n",
        "                if len(replay_buffer) > initial_buffer_size:\n",
        "                    update(batch_size, beta_func(episode))\n",
        "\n",
        "                # ターゲットネットワークを定期的に同期させる\n",
        "                if (update_step + 1) % target_update_interval == 0:\n",
        "                    print(\"target_net update\")\n",
        "                    target_net.load_state_dict(net.state_dict())\n",
        "\n",
        "                update_step += 1\n",
        "\n",
        "                if terminated or step == MAX_STEPS -1 or truncated:  # ゴールに到達した場合 or 最大ステップ数に到達した場合\n",
        "\n",
        "                    if truncated:\n",
        "                      print(\"truncated!!\")\n",
        "                    else:\n",
        "                      print(\"terminated!!\")\n",
        "\n",
        "                    self.reward_mean = np.hstack((self.reward_mean[1:], self.rewards))\n",
        "                    print(f'{episode + 1} Episode: Finished after {step + 1} time steps, rewards  = {self.rewards} reward mean = {self.reward_mean.mean()}')\n",
        "                    sum_reward.append(self.rewards)\n",
        "                    self.env.save_video()\n",
        "                    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPbNRnhGd4jc"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Va3AJDFWd4jd",
        "outputId": "d7156e3b-8d12-444e-f11a-1720b1fe023f"
      },
      "outputs": [],
      "source": [
        "roomba_env = LearningAgent(env=env)\n",
        "roomba_env.run()\n",
        "# roomba_env.env.roomba_simulator.save_video()\n",
        "# roomba_env.env.roomba_simulator.display_video()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ou2mvJd5a3zp"
      },
      "outputs": [],
      "source": [
        "roomba_env.env.save_video()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ji938iqRTXsj"
      },
      "source": [
        "# Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEo2JRJqTfHn"
      },
      "outputs": [],
      "source": [
        "# 値の推移をプロット\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(sum_reward, label=\"Reward\", color=\"red\", alpha=0.5)\n",
        "\n",
        "\n",
        "# 移動平均の推移を計算してプロット\n",
        "window_size = 100  # 移動平均の窓サイズ\n",
        "\n",
        "# リストをarrayに変換\n",
        "arr = np.array(sum_reward)\n",
        "\n",
        "# 移動平均を計算\n",
        "kernel = np.ones(window_size) / window_size\n",
        "rolling_mean = np.convolve(arr, kernel, mode='valid')\n",
        "\n",
        "plt.plot(rolling_mean, label=f\"Reward moving Average ({window_size})\", color=\"red\")\n",
        "plt.rcParams[\"font.size\"] = 15\n",
        "# グラフのタイトルとラベル\n",
        "# plt.title('Value Transition and Moving Average')\n",
        "plt.xlabel(\"episodes\")\n",
        "plt.ylabel(\"reward\")\n",
        "plt.legend(loc='lower left')\n",
        "\n",
        "# グラフを表示\n",
        "plt.tick_params(direction='in')  # 目盛りを内側にする\n",
        "plt.tight_layout()  # レイアウトを調整して重なりを避ける\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1S8x--0ravoy"
      },
      "source": [
        "# Save model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVta-QcMa3an"
      },
      "outputs": [],
      "source": [
        "model_dir = 'model'\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "torch.save(net.state_dict(), './model/model12.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s25fQ6Yfa5ts"
      },
      "source": [
        "# load model weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCVdfu02ayxV"
      },
      "outputs": [],
      "source": [
        "# model = CNNQNetwork(env.observation_space.shape, n_action=env.action_space.n)\n",
        "# model.load_state_dict(torch.load('load/from/path/model.pth'))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
